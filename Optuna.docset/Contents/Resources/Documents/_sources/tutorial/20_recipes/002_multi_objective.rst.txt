
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorial/20_recipes/002_multi_objective.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorial_20_recipes_002_multi_objective.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorial_20_recipes_002_multi_objective.py:


.. _multi_objective:

Multi-objective Optimization with Optuna
========================================

This tutorial showcases Optuna's multi-objective optimization feature by
optimizing the validation accuracy of Fashion MNIST dataset and the FLOPS of the model implemented in PyTorch.

We use `thop <https://github.com/Lyken17/pytorch-OpCounter>`_ to measure FLOPS.

.. GENERATED FROM PYTHON SOURCE LINES 12-74

.. code-block:: default


    import thop
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision

    import optuna


    DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    DIR = ".."
    BATCHSIZE = 128
    N_TRAIN_EXAMPLES = BATCHSIZE * 30
    N_VALID_EXAMPLES = BATCHSIZE * 10


    def define_model(trial):
        n_layers = trial.suggest_int("n_layers", 1, 3)
        layers = []

        in_features = 28 * 28
        for i in range(n_layers):
            out_features = trial.suggest_int("n_units_l{}".format(i), 4, 128)
            layers.append(nn.Linear(in_features, out_features))
            layers.append(nn.ReLU())
            p = trial.suggest_float("dropout_{}".format(i), 0.2, 0.5)
            layers.append(nn.Dropout(p))

            in_features = out_features

        layers.append(nn.Linear(in_features, 10))
        layers.append(nn.LogSoftmax(dim=1))

        return nn.Sequential(*layers)


    # Defines training and evaluation.
    def train_model(model, optimizer, train_loader):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)
            optimizer.zero_grad()
            F.nll_loss(model(data), target).backward()
            optimizer.step()


    def eval_model(model, valid_loader):
        model.eval()
        correct = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(valid_loader):
                data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)
                pred = model(data).argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        accuracy = correct / N_VALID_EXAMPLES

        flops, _ = thop.profile(model, inputs=(torch.randn(1, 28 * 28).to(DEVICE),), verbose=False)
        return flops, accuracy









.. GENERATED FROM PYTHON SOURCE LINES 75-77

Define multi-objective objective function.
Objectives are FLOPS and accuracy.

.. GENERATED FROM PYTHON SOURCE LINES 77-107

.. code-block:: default

    def objective(trial):
        train_dataset = torchvision.datasets.FashionMNIST(
            DIR, train=True, download=True, transform=torchvision.transforms.ToTensor()
        )
        train_loader = torch.utils.data.DataLoader(
            torch.utils.data.Subset(train_dataset, list(range(N_TRAIN_EXAMPLES))),
            batch_size=BATCHSIZE,
            shuffle=True,
        )

        val_dataset = torchvision.datasets.FashionMNIST(
            DIR, train=False, transform=torchvision.transforms.ToTensor()
        )
        val_loader = torch.utils.data.DataLoader(
            torch.utils.data.Subset(val_dataset, list(range(N_VALID_EXAMPLES))),
            batch_size=BATCHSIZE,
            shuffle=True,
        )
        model = define_model(trial).to(DEVICE)

        optimizer = torch.optim.Adam(
            model.parameters(), trial.suggest_float("lr", 1e-5, 1e-1, log=True)
        )

        for epoch in range(10):
            train_model(model, optimizer, train_loader)
        flops, accuracy = eval_model(model, val_loader)
        return flops, accuracy









.. GENERATED FROM PYTHON SOURCE LINES 108-115

Run multi-objective optimization
--------------------------------

If your optimization problem is multi-objective,
Optuna assumes that you will specify the optimization direction for each objective.
Specifically, in this example, we want to minimize the FLOPS (we want a faster model)
and maximize the accuracy. So we set ``directions`` to ``["minimize", "maximize"]``.

.. GENERATED FROM PYTHON SOURCE LINES 115-121

.. code-block:: default

    study = optuna.create_study(directions=["minimize", "maximize"])
    study.optimize(objective, n_trials=30, timeout=300)

    print("Number of finished trials: ", len(study.trials))






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../FashionMNIST/raw/train-images-idx3-ubyte.gz
      0%|          | 0/26421880 [00:00<?, ?it/s]      0%|          | 40960/26421880 [00:00<01:07, 389515.85it/s]      0%|          | 103424/26421880 [00:00<01:10, 374828.65it/s]      1%|1         | 394240/26421880 [00:00<00:20, 1281487.23it/s]      4%|3         | 946176/26421880 [00:00<00:11, 2203391.09it/s]     12%|#1        | 3075072/26421880 [00:00<00:03, 7591649.68it/s]     22%|##2       | 5865472/26421880 [00:00<00:01, 13429402.93it/s]     35%|###5      | 9271296/26421880 [00:00<00:01, 16232939.16it/s]     49%|####9     | 12957696/26421880 [00:01<00:00, 21550852.96it/s]     60%|#####9    | 15776768/26421880 [00:01<00:00, 23337675.43it/s]     73%|#######3  | 19302400/26421880 [00:01<00:00, 22431352.31it/s]     88%|########7 | 23208960/26421880 [00:01<00:00, 26595674.52it/s]     99%|#########8| 26143744/26421880 [00:01<00:00, 27320357.99it/s]    26422272it [00:01, 17755934.09it/s]                              
    Extracting ../FashionMNIST/raw/train-images-idx3-ubyte.gz to ../FashionMNIST/raw
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../FashionMNIST/raw/train-labels-idx1-ubyte.gz
      0%|          | 0/29515 [00:00<?, ?it/s]    29696it [00:00, 319952.66it/s]           
    Extracting ../FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../FashionMNIST/raw
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../FashionMNIST/raw/t10k-images-idx3-ubyte.gz
      0%|          | 0/4422102 [00:00<?, ?it/s]      1%|          | 40960/4422102 [00:00<00:11, 366330.38it/s]      2%|2         | 104448/4422102 [00:00<00:11, 375987.30it/s]      7%|7         | 316416/4422102 [00:00<00:04, 973031.46it/s]     19%|#9        | 855040/4422102 [00:00<00:01, 2003220.60it/s]     59%|#####8    | 2606080/4422102 [00:00<00:00, 6126890.82it/s]    4422656it [00:00, 5890865.31it/s]                             
    Extracting ../FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../FashionMNIST/raw
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
      0%|          | 0/5148 [00:00<?, ?it/s]    6144it [00:00, 40139881.27it/s]         
    Extracting ../FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../FashionMNIST/raw
    Processing...
    /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/torchvision/datasets/mnist.py:479: UserWarning:

    The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)

    Done!
    Number of finished trials:  30




.. GENERATED FROM PYTHON SOURCE LINES 122-123

Check trials on pareto front visually

.. GENERATED FROM PYTHON SOURCE LINES 123-126

.. code-block:: default

    optuna.visualization.plot_pareto_front(study, target_names=["FLOPS", "accuracy"])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/dash-docset-optuna/dash-docset-optuna/optuna/tutorial/20_recipes/002_multi_objective.py:123: ExperimentalWarning:

    plot_pareto_front is experimental (supported from v2.4.0). The interface can change in the future.



.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
            <script src="https://cdn.plot.ly/plotly-2.12.1.min.js"></script>                <div id="4e970694-9d49-423a-b4e5-14485d4bfee1" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("4e970694-9d49-423a-b4e5-14485d4bfee1")) {                    Plotly.newPlot(                        "4e970694-9d49-423a-b4e5-14485d4bfee1",                        [{"hovertemplate":"%{text}<extra>Trial</extra>","marker":{"color":[0,1,2,3,4,5,7,10,11,12,13,14,15,16,19,21,23,24,25,26,27,28],"colorbar":{"title":{"text":"#Trials"}},"colorscale":[[0.0,"rgb(247,251,255)"],[0.125,"rgb(222,235,247)"],[0.25,"rgb(198,219,239)"],[0.375,"rgb(158,202,225)"],[0.5,"rgb(107,174,214)"],[0.625,"rgb(66,146,198)"],[0.75,"rgb(33,113,181)"],[0.875,"rgb(8,81,156)"],[1.0,"rgb(8,48,107)"]],"line":{"color":"Grey","width":0.5}},"mode":"markers","showlegend":false,"text":["{<br>  \"number\": 0,<br>  \"values\": [<br>    41579.0,<br>    0.1828125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 47,<br>    \"dropout_0\": 0.2686414050849425,<br>    \"n_units_l1\": 83,<br>    \"dropout_1\": 0.45258533550052693,<br>    \"lr\": 1.1283039638658155e-05<br>  }<br>}","{<br>  \"number\": 1,<br>  \"values\": [<br>    70776.0,<br>    0.08984375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 78,<br>    \"dropout_0\": 0.4406092510444154,<br>    \"n_units_l1\": 92,<br>    \"dropout_1\": 0.23865716197635695,<br>    \"n_units_l2\": 24,<br>    \"dropout_2\": 0.38089435559938184,<br>    \"lr\": 0.09506869400512982<br>  }<br>}","{<br>  \"number\": 2,<br>  \"values\": [<br>    93863.0,<br>    0.35625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 92,<br>    \"dropout_0\": 0.4294237522516675,<br>    \"n_units_l1\": 105,<br>    \"dropout_1\": 0.26329592902939414,<br>    \"n_units_l2\": 105,<br>    \"dropout_2\": 0.237403516397304,<br>    \"lr\": 2.0454883993943466e-05<br>  }<br>}","{<br>  \"number\": 3,<br>  \"values\": [<br>    22462.0,<br>    0.6046875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 21,<br>    \"dropout_0\": 0.4753047146096289,<br>    \"n_units_l1\": 54,<br>    \"dropout_1\": 0.22749273154540234,<br>    \"n_units_l2\": 76,<br>    \"dropout_2\": 0.2628536184897556,<br>    \"lr\": 0.0003177370283427409<br>  }<br>}","{<br>  \"number\": 4,<br>  \"values\": [<br>    55070.0,<br>    0.46875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 65,<br>    \"dropout_0\": 0.4935913355775346,<br>    \"n_units_l1\": 46,<br>    \"dropout_1\": 0.21042776125836588,<br>    \"n_units_l2\": 20,<br>    \"dropout_2\": 0.43656281985999595,<br>    \"lr\": 0.0001168004996810423<br>  }<br>}","{<br>  \"number\": 5,<br>  \"values\": [<br>    27624.0,<br>    0.521875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 16,<br>    \"dropout_0\": 0.39653507217851314,<br>    \"n_units_l1\": 110,<br>    \"dropout_1\": 0.26773379430214195,<br>    \"n_units_l2\": 111,<br>    \"dropout_2\": 0.33006242845577694,<br>    \"lr\": 0.00010222928312929415<br>  }<br>}","{<br>  \"number\": 7,<br>  \"values\": [<br>    35574.0,<br>    0.56875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 39,<br>    \"dropout_0\": 0.23531339100014526,<br>    \"n_units_l1\": 102,<br>    \"dropout_1\": 0.4552275225428235,<br>    \"lr\": 5.1353165307667126e-05<br>  }<br>}","{<br>  \"number\": 10,<br>  \"values\": [<br>    47505.0,<br>    0.7703125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 48,<br>    \"dropout_0\": 0.44965106003566585,<br>    \"n_units_l1\": 109,<br>    \"dropout_1\": 0.2451620123168125,<br>    \"n_units_l2\": 39,<br>    \"dropout_2\": 0.3504469002707672,<br>    \"lr\": 0.0027284396471782303<br>  }<br>}","{<br>  \"number\": 11,<br>  \"values\": [<br>    92898.0,<br>    0.77734375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 117,<br>    \"dropout_0\": 0.44915589695457137,<br>    \"lr\": 0.0003580049831246258<br>  }<br>}","{<br>  \"number\": 12,<br>  \"values\": [<br>    34796.0,<br>    0.61484375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 41,<br>    \"dropout_0\": 0.2725923960879143,<br>    \"n_units_l1\": 52,<br>    \"dropout_1\": 0.386220079948006,<br>    \"lr\": 7.654826578935709e-05<br>  }<br>}","{<br>  \"number\": 13,<br>  \"values\": [<br>    89696.0,<br>    0.58984375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 111,<br>    \"dropout_0\": 0.36028924335682083,<br>    \"n_units_l1\": 21,<br>    \"dropout_1\": 0.35549144813114186,<br>    \"n_units_l2\": 11,<br>    \"dropout_2\": 0.3413134609036135,<br>    \"lr\": 0.0004865037123430337<br>  }<br>}","{<br>  \"number\": 14,<br>  \"values\": [<br>    59800.0,<br>    0.771875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 56,<br>    \"dropout_0\": 0.3570104345797347,<br>    \"n_units_l1\": 111,<br>    \"dropout_1\": 0.30229567354876435,<br>    \"n_units_l2\": 80,<br>    \"dropout_2\": 0.2868312357826866,<br>    \"lr\": 0.0007196942477936411<br>  }<br>}","{<br>  \"number\": 15,<br>  \"values\": [<br>    112693.0,<br>    0.33515625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 124,<br>    \"dropout_0\": 0.39030006471264794,<br>    \"n_units_l1\": 63,<br>    \"dropout_1\": 0.34631697079491974,<br>    \"n_units_l2\": 105,<br>    \"dropout_2\": 0.29807447853323993,<br>    \"lr\": 2.7565888413534448e-05<br>  }<br>}","{<br>  \"number\": 16,<br>  \"values\": [<br>    30966.0,<br>    0.7765625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 39,<br>    \"dropout_0\": 0.2824477450983745,<br>    \"lr\": 0.0006508461488577301<br>  }<br>}","{<br>  \"number\": 19,<br>  \"values\": [<br>    73048.0,<br>    0.815625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 92,<br>    \"dropout_0\": 0.4366844551989252,<br>    \"lr\": 0.010464302249069399<br>  }<br>}","{<br>  \"number\": 21,<br>  \"values\": [<br>    90458.0,<br>    0.72578125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 112,<br>    \"dropout_0\": 0.306814314644945,<br>    \"n_units_l1\": 19,<br>    \"dropout_1\": 0.42652594648828046,<br>    \"n_units_l2\": 18,<br>    \"dropout_2\": 0.37875679236873416,<br>    \"lr\": 0.002363709478561505<br>  }<br>}","{<br>  \"number\": 23,<br>  \"values\": [<br>    81432.0,<br>    0.59140625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 88,<br>    \"dropout_0\": 0.46876778720855716,<br>    \"n_units_l1\": 80,<br>    \"dropout_1\": 0.488272060495866,<br>    \"n_units_l2\": 60,<br>    \"dropout_2\": 0.36646672079542975,<br>    \"lr\": 0.025781858806695257<br>  }<br>}","{<br>  \"number\": 24,<br>  \"values\": [<br>    46466.0,<br>    0.5171875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 53,<br>    \"dropout_0\": 0.32033237205207526,<br>    \"n_units_l1\": 78,<br>    \"dropout_1\": 0.43260225650831163,<br>    \"lr\": 3.198475939001599e-05<br>  }<br>}","{<br>  \"number\": 25,<br>  \"values\": [<br>    25408.0,<br>    0.25234375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 32,<br>    \"dropout_0\": 0.4842014653352208,<br>    \"lr\": 0.09886203831971052<br>  }<br>}","{<br>  \"number\": 26,<br>  \"values\": [<br>    98640.0,<br>    0.76015625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 111,<br>    \"dropout_0\": 0.4231782761002574,<br>    \"n_units_l1\": 96,<br>    \"dropout_1\": 0.46721483796711843,<br>    \"lr\": 0.0003492286876114118<br>  }<br>}","{<br>  \"number\": 27,<br>  \"values\": [<br>    21854.0,<br>    0.1015625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 11,<br>    \"dropout_0\": 0.3990381447517127,<br>    \"n_units_l1\": 105,<br>    \"dropout_1\": 0.22647653024660142,<br>    \"n_units_l2\": 105,<br>    \"dropout_2\": 0.24355073223069798,<br>    \"lr\": 0.040958747446614936<br>  }<br>}","{<br>  \"number\": 28,<br>  \"values\": [<br>    61460.0,<br>    0.66796875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 74,<br>    \"dropout_0\": 0.3817225137101135,<br>    \"n_units_l1\": 41,<br>    \"dropout_1\": 0.38396332571302694,<br>    \"lr\": 0.03671159140587022<br>  }<br>}"],"x":[41579.0,70776.0,93863.0,22462.0,55070.0,27624.0,35574.0,47505.0,92898.0,34796.0,89696.0,59800.0,112693.0,30966.0,73048.0,90458.0,81432.0,46466.0,25408.0,98640.0,21854.0,61460.0],"y":[0.1828125,0.08984375,0.35625,0.6046875,0.46875,0.521875,0.56875,0.7703125,0.77734375,0.61484375,0.58984375,0.771875,0.33515625,0.7765625,0.815625,0.72578125,0.59140625,0.5171875,0.25234375,0.76015625,0.1015625,0.66796875],"type":"scatter"},{"hovertemplate":"%{text}<extra>Best Trial</extra>","marker":{"color":[6,8,9,17,18,20,22,29],"colorbar":{"title":{"text":"#Best trials"},"x":1.1,"xpad":40},"colorscale":[[0.0,"rgb(255,245,240)"],[0.125,"rgb(254,224,210)"],[0.25,"rgb(252,187,161)"],[0.375,"rgb(252,146,114)"],[0.5,"rgb(251,106,74)"],[0.625,"rgb(239,59,44)"],[0.75,"rgb(203,24,29)"],[0.875,"rgb(165,15,21)"],[1.0,"rgb(103,0,13)"]],"line":{"color":"Grey","width":0.5}},"mode":"markers","showlegend":false,"text":["{<br>  \"number\": 6,<br>  \"values\": [<br>    63520.0,<br>    0.79921875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 80,<br>    \"dropout_0\": 0.2703854648688738,<br>    \"lr\": 0.0005522665842008342<br>  }<br>}","{<br>  \"number\": 8,<br>  \"values\": [<br>    83370.0,<br>    0.834375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 105,<br>    \"dropout_0\": 0.4513207336138845,<br>    \"lr\": 0.0018506116134321352<br>  }<br>}","{<br>  \"number\": 9,<br>  \"values\": [<br>    58434.0,<br>    0.78828125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 3,<br>    \"n_units_l0\": 61,<br>    \"dropout_0\": 0.46736677953602346,<br>    \"n_units_l1\": 122,<br>    \"dropout_1\": 0.46374001946883237,<br>    \"n_units_l2\": 24,<br>    \"dropout_2\": 0.2689922573922243,<br>    \"lr\": 0.005584082771811796<br>  }<br>}","{<br>  \"number\": 17,<br>  \"values\": [<br>    19056.0,<br>    0.7828125<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 24,<br>    \"dropout_0\": 0.44014729415238746,<br>    \"lr\": 0.0035992506015758135<br>  }<br>}","{<br>  \"number\": 18,<br>  \"values\": [<br>    17388.0,<br>    0.6296875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 18,<br>    \"dropout_0\": 0.4644710883810354,<br>    \"n_units_l1\": 117,<br>    \"dropout_1\": 0.24813793874289775,<br>    \"lr\": 0.00017063171219025127<br>  }<br>}","{<br>  \"number\": 20,<br>  \"values\": [<br>    11641.0,<br>    0.55546875<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 13,<br>    \"dropout_0\": 0.2621974476906459,<br>    \"n_units_l1\": 63,<br>    \"dropout_1\": 0.4626420072341303,<br>    \"lr\": 0.03640544121338093<br>  }<br>}","{<br>  \"number\": 22,<br>  \"values\": [<br>    81782.0,<br>    0.82890625<br>  ],<br>  \"params\": {<br>    \"n_layers\": 1,<br>    \"n_units_l0\": 103,<br>    \"dropout_0\": 0.3727252574523003,<br>    \"lr\": 0.008493253329156125<br>  }<br>}","{<br>  \"number\": 29,<br>  \"values\": [<br>    68700.0,<br>    0.8234375<br>  ],<br>  \"params\": {<br>    \"n_layers\": 2,<br>    \"n_units_l0\": 79,<br>    \"dropout_0\": 0.3036668345355079,<br>    \"n_units_l1\": 76,<br>    \"dropout_1\": 0.3439839540519455,<br>    \"lr\": 0.00579952908067237<br>  }<br>}"],"x":[63520.0,83370.0,58434.0,19056.0,17388.0,11641.0,81782.0,68700.0],"y":[0.79921875,0.834375,0.78828125,0.7828125,0.6296875,0.55546875,0.82890625,0.8234375],"type":"scatter"}],                        {"title":{"text":"Pareto-front Plot"},"xaxis":{"title":{"text":"FLOPS"}},"yaxis":{"title":{"text":"accuracy"}},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    )                };                            </script>        </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 127-128

Learn which hyperparameters are affecting the flops most with hyperparameter importance.

.. GENERATED FROM PYTHON SOURCE LINES 128-131

.. code-block:: default

    optuna.visualization.plot_param_importances(
        study, target=lambda t: t.values[0], target_name="flops"
    )





.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
            <script src="https://cdn.plot.ly/plotly-2.12.1.min.js"></script>                <div id="74835457-f636-4519-ba3a-39c811261604" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("74835457-f636-4519-ba3a-39c811261604")) {                    Plotly.newPlot(                        "74835457-f636-4519-ba3a-39c811261604",                        [{"cliponaxis":false,"hovertemplate":["n_layers (IntUniformDistribution): 0.0009471919135073229<extra></extra>","lr (LogUniformDistribution): 0.0029027673138748815<extra></extra>","dropout_0 (UniformDistribution): 0.0043019653718685425<extra></extra>","n_units_l0 (IntUniformDistribution): 0.9918480754007492<extra></extra>"],"marker":{"color":"rgb(66,146,198)"},"orientation":"h","text":["0.0009471919135073229","0.0029027673138748815","0.0043019653718685425","0.9918480754007492"],"textposition":"outside","texttemplate":"%{text:.2f}","x":[0.0009471919135073229,0.0029027673138748815,0.0043019653718685425,0.9918480754007492],"y":["n_layers","lr","dropout_0","n_units_l0"],"type":"bar"}],                        {"showlegend":false,"title":{"text":"Hyperparameter Importances"},"xaxis":{"title":{"text":"Importance for flops"}},"yaxis":{"title":{"text":"Hyperparameter"}},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    )                };                            </script>        </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  28.185 seconds)


.. _sphx_glr_download_tutorial_20_recipes_002_multi_objective.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 002_multi_objective.py <002_multi_objective.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 002_multi_objective.ipynb <002_multi_objective.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
